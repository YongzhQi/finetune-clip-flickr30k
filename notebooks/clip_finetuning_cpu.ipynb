{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38cebe18",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-tune CLIP (ViT-B/32) on Flickr30k (CPU)\n",
    "\n",
    "This notebook guides you through running a CPU-only linear-probe style fine-tuning\n",
    "of OpenAI's CLIP model on the Flickr30k dataset. You can either rely on the\n",
    "Hugging Face dataset loader (when accessible) or provide a local TSV file with\n",
    "image/caption metadata. The notebook evaluates Recall@1/5/10 and median rank for\n",
    "both imageâ†’text and textâ†’image retrieval, comparing the raw (zero-shot) CLIP\n",
    "model against the fine-tuned checkpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e7321",
   "metadata": {},
   "source": [
    "\n",
    "## What You'll Get\n",
    "\n",
    "- CPU-friendly fine-tuning that updates only CLIP projection heads plus the\n",
    "  temperature parameter.\n",
    "- Retrieval metrics (Recall@1/5/10 and Median Rank) for both imageâ†’text and\n",
    "  textâ†’image directions.\n",
    "- Built-in comparison tables and JSON export showing zero-shot vs fine-tuned\n",
    "  performance.\n",
    "- Optional TSV-based data ingestion if you prefer a manifest-driven workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e05829",
   "metadata": {},
   "source": [
    "\n",
    "## Installation (CPU)\n",
    "\n",
    "Install dependencies in a CPU environment (recommended to use a fresh virtualenv\n",
    "or conda env):\n",
    "\n",
    "```bash\n",
    "python -m pip install --upgrade pip\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "pip install open_clip_torch datasets pillow tqdm numpy faiss-cpu\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d11a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment and run the following lines if you still need to install dependencies.\n",
    "#!python -m pip install --upgrade pip\n",
    "#!pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "#!pip install open_clip_torch datasets pillow tqdm numpy faiss-cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc25eb4",
   "metadata": {},
   "source": [
    "## Data Options\n",
    "\n",
    "1. **Torchvision dataset (recommended):** Point to your Flickr30k image directory, the\n",
    "   official `captions.txt`, and optionally a directory/file containing split lists\n",
    "   (`train.txt`, `val.txt`, `test.txt`).\n",
    "2. **Local TSV fallback:** Provide a tab-separated file with columns\n",
    "   `image_path`, `caption`, `image_id`, `split`. Each image should appear five times\n",
    "   (one per caption). This is handy for custom subsets or pre-filtered data.\n",
    "\n",
    "Pick exactly one path (torchvision *or* TSV) per run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa362828",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9590d7",
   "metadata": {},
   "source": [
    "## Imports and Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a689658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import open_clip\n",
    "\n",
    "try:\n",
    "    from torchvision.datasets import Flickr30k as TVFlickr30k\n",
    "    from torchvision.datasets.folder import default_loader as tv_default_loader\n",
    "except ImportError:\n",
    "    TVFlickr30k = None\n",
    "    tv_default_loader = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecf29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    image: torch.Tensor\n",
    "    tokens: torch.Tensor\n",
    "    image_id: str\n",
    "    caption: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fedcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchvisionFlickr(Dataset):\n",
    "    \"\"\"Dataset backed by torchvision Flickr30k annotations (images + captions).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str | os.PathLike[str],\n",
    "        ann_file: str | os.PathLike[str],\n",
    "        split: str,\n",
    "        preprocess,\n",
    "        tokenizer,\n",
    "        subset: Optional[int] = None,\n",
    "        split_file: str | os.PathLike[str] | None = None,\n",
    "    ) -> None:\n",
    "        if tv_default_loader is None:\n",
    "            raise ImportError(\n",
    "                \"torchvision is required for TorchvisionFlickr. Install torchvision first.\"\n",
    "            )\n",
    "\n",
    "        self.root = Path(root)\n",
    "        self.ann_file = Path(ann_file)\n",
    "        self.preprocess = preprocess\n",
    "        self.tokenizer = tokenizer\n",
    "        self.loader = tv_default_loader\n",
    "\n",
    "        if not self.root.exists():\n",
    "            raise FileNotFoundError(f\"Image root not found: {root}\")\n",
    "        if not self.ann_file.is_file():\n",
    "            raise FileNotFoundError(f\"Annotation file not found: {ann_file}\")\n",
    "\n",
    "        allowed_images: Optional[set[str]] = None\n",
    "        split_lower = split.lower()\n",
    "        candidate_paths: List[Path] = []\n",
    "        if split_file:\n",
    "            candidate_paths.append(Path(split_file))\n",
    "        candidate_paths.extend(\n",
    "            [\n",
    "                self.ann_file.with_name(f\"{split_lower}.txt\"),\n",
    "                self.ann_file.with_name(f\"{split_lower}.lst\"),\n",
    "                self.ann_file.with_name(f\"{split_lower}.list\"),\n",
    "                self.root / f\"{split_lower}.txt\",\n",
    "                self.root / \"splits\" / f\"{split_lower}.txt\",\n",
    "                self.root / \"splits\" / f\"{split_lower}.list\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for candidate in candidate_paths:\n",
    "            if candidate.is_file():\n",
    "                allowed_images = self._read_split_file(candidate)\n",
    "                break\n",
    "\n",
    "        captions_map: Dict[str, List[str]] = {}\n",
    "        with self.ann_file.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "            for line in handle:\n",
    "                line = line.strip()\n",
    "                if not line or \"\t\" not in line:\n",
    "                    continue\n",
    "                key, caption = line.split(\"\t\", 1)\n",
    "                image_name = key.split(\"#\", 1)[0]\n",
    "                captions_map.setdefault(image_name, []).append(caption)\n",
    "\n",
    "        rows: List[Tuple[Path, str, str]] = []\n",
    "        for image_name, captions in captions_map.items():\n",
    "            normalized = image_name if image_name.lower().endswith(\".jpg\") else f\"{image_name}.jpg\"\n",
    "            if allowed_images and normalized not in allowed_images:\n",
    "                continue\n",
    "            image_path = self._resolve_image_path(normalized)\n",
    "            if image_path is None:\n",
    "                continue\n",
    "            for caption in captions:\n",
    "                if not caption:\n",
    "                    continue\n",
    "                rows.append((image_path, caption, normalized))\n",
    "\n",
    "        if subset and subset > 0:\n",
    "            rows = rows[:subset]\n",
    "\n",
    "        if not rows:\n",
    "            raise ValueError(\n",
    "                f\"No rows found for split='{split}' using torchvision Flickr30k annotations.\"\n",
    "            )\n",
    "\n",
    "        self.rows = rows\n",
    "\n",
    "    def _read_split_file(self, path: Path) -> set[str]:\n",
    "        allowed: set[str] = set()\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "            for line in handle:\n",
    "                entry = line.strip().split()[0]\n",
    "                if not entry:\n",
    "                    continue\n",
    "                if not entry.lower().endswith(\".jpg\"):\n",
    "                    entry = f\"{entry}.jpg\"\n",
    "                allowed.add(entry)\n",
    "        return allowed\n",
    "\n",
    "    def _resolve_image_path(self, image_name: str) -> Path | None:\n",
    "        candidates = [\n",
    "            self.root / image_name,\n",
    "            self.root / \"flickr30k_images\" / image_name,\n",
    "            self.root / \"flickr30k-images\" / image_name,\n",
    "            self.root / \"Images\" / image_name,\n",
    "        ]\n",
    "        for candidate in candidates:\n",
    "            if candidate.is_file():\n",
    "                return candidate\n",
    "        return None\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Sample:\n",
    "        image_path, caption, image_id = self.rows[index]\n",
    "        image = self.loader(str(image_path))\n",
    "        if not isinstance(image, Image.Image):\n",
    "            raise TypeError(\"torchvision default_loader did not return a PIL.Image\")\n",
    "        image = self.preprocess(image.convert(\"RGB\"))\n",
    "        tokens = self.tokenizer([caption])[0]\n",
    "        return Sample(image=image, tokens=tokens, image_id=image_id, caption=caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0741e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate(batch: Sequence[Sample]) -> Tuple[torch.Tensor, torch.Tensor, List[str], List[str]]:\n",
    "    images = torch.stack([item.image for item in batch], dim=0)\n",
    "    tokens = torch.stack([item.tokens for item in batch], dim=0)\n",
    "    image_ids = [item.image_id for item in batch]\n",
    "    captions = [item.caption for item in batch]\n",
    "    return images, tokens, image_ids, captions\n",
    "\n",
    "\n",
    "def recall_at_k(similarity: np.ndarray, gt_sets: Sequence[Sequence[int]], ks: Iterable[int] = (1, 5, 10)) -> dict:\n",
    "    if similarity.ndim != 2:\n",
    "        raise ValueError('similarity matrix must be 2D')\n",
    "\n",
    "    num_images = similarity.shape[0]\n",
    "    order = np.argsort(-similarity, axis=1)\n",
    "    ranks: List[int] = []\n",
    "    for idx in range(num_images):\n",
    "        gt = gt_sets[idx]\n",
    "        rank_candidates = [int(np.where(order[idx] == gt_idx)[0][0]) for gt_idx in gt]\n",
    "        ranks.append(min(rank_candidates))\n",
    "\n",
    "    ranks_array = np.array(ranks, dtype=np.int64)\n",
    "    metrics = {f'R@{k}': float((ranks_array < k).mean() * 100.0) for k in ks}\n",
    "    metrics['MedR'] = float(np.median(ranks_array) + 1.0)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, dataloader: DataLoader, device: str = 'cpu') -> dict:\n",
    "    model.eval()\n",
    "    img_features: List[np.ndarray] = []\n",
    "    txt_features: List[np.ndarray] = []\n",
    "    caption_image_ids: List[str] = []\n",
    "    image_id_to_index: dict[str, int] = {}\n",
    "    unique_image_ids: List[str] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, tokens, image_ids, _ in tqdm(dataloader, desc='Encode eval', leave=False):\n",
    "            images = images.to(device)\n",
    "            tokens = tokens.to(device)\n",
    "\n",
    "            image_feats = model.encode_image(images)\n",
    "            text_feats = model.encode_text(tokens)\n",
    "            image_feats = image_feats / image_feats.norm(dim=-1, keepdim=True)\n",
    "            text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            for j in range(text_feats.size(0)):\n",
    "                txt_features.append(text_feats[j].cpu().numpy())\n",
    "                caption_image_ids.append(image_ids[j])\n",
    "\n",
    "            for j in range(image_feats.size(0)):\n",
    "                iid = image_ids[j]\n",
    "                if iid not in image_id_to_index:\n",
    "                    image_id_to_index[iid] = len(unique_image_ids)\n",
    "                    unique_image_ids.append(iid)\n",
    "                    img_features.append(image_feats[j].cpu().numpy())\n",
    "\n",
    "    if not img_features or not txt_features:\n",
    "        raise ValueError('Evaluation dataset produced no features')\n",
    "\n",
    "    img_matrix = np.stack(img_features, axis=0)\n",
    "    txt_matrix = np.stack(txt_features, axis=0)\n",
    "\n",
    "    gt_sets: List[List[int]] = [list() for _ in unique_image_ids]\n",
    "    for caption_idx, iid in enumerate(caption_image_ids):\n",
    "        gt_sets[image_id_to_index[iid]].append(caption_idx)\n",
    "\n",
    "    sim_i2t = img_matrix @ txt_matrix.T\n",
    "    i2t_metrics = recall_at_k(sim_i2t, gt_sets)\n",
    "\n",
    "    sim_t2i = txt_matrix @ img_matrix.T\n",
    "    order_t2i = np.argsort(-sim_t2i, axis=1)\n",
    "    ranks = []\n",
    "    for caption_idx, iid in enumerate(caption_image_ids):\n",
    "        target = image_id_to_index[iid]\n",
    "        rank = int(np.where(order_t2i[caption_idx] == target)[0][0])\n",
    "        ranks.append(rank)\n",
    "    ranks_array = np.array(ranks, dtype=np.int64)\n",
    "    t2i_metrics = {f'R@{k}': float((ranks_array < k).mean() * 100.0) for k in (1, 5, 10)}\n",
    "    t2i_metrics['MedR'] = float(np.median(ranks_array) + 1.0)\n",
    "\n",
    "    return {'I->T': i2t_metrics, 'T->I': t2i_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: str = 'cpu',\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    batches = 0\n",
    "\n",
    "    for images, tokens, _, _ in tqdm(dataloader, desc='Train', leave=False):\n",
    "        images = images.to(device)\n",
    "        tokens = tokens.to(device)\n",
    "\n",
    "        image_feats = model.encode_image(images)\n",
    "        text_feats = model.encode_text(tokens)\n",
    "        image_feats = image_feats / image_feats.norm(dim=-1, keepdim=True)\n",
    "        text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "        logits = logit_scale * image_feats @ text_feats.t()\n",
    "        labels = torch.arange(logits.size(0), device=device)\n",
    "\n",
    "        loss_i2t = nn.CrossEntropyLoss()(logits, labels)\n",
    "        loss_t2i = nn.CrossEntropyLoss()(logits.t(), labels)\n",
    "        loss = 0.5 * (loss_i2t + loss_t2i)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_([p for p in model.parameters() if p.requires_grad], 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.logit_scale.data.clamp_(math.log(1 / 0.07), math.log(100.0))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batches += 1\n",
    "\n",
    "    return total_loss / max(batches, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _flatten_metrics(metrics: dict) -> dict:\n",
    "    flat: dict[str, float] = {}\n",
    "    for direction in ('I->T', 'T->I'):\n",
    "        for metric, value in metrics[direction].items():\n",
    "            flat[f'{direction}_{metric}'] = float(value)\n",
    "    return flat\n",
    "\n",
    "\n",
    "def _compare_and_print(split_name: str, baseline: dict, tuned: dict) -> None:\n",
    "    def fmt(value: float) -> str:\n",
    "        return f'{value:6.2f}'\n",
    "\n",
    "    print('\n",
    "' + '=' * 64)\n",
    "    print(f'{split_name} â€” Raw CLIP vs Fine-tuned CLIP')\n",
    "    print('=' * 64)\n",
    "\n",
    "    for direction in ('I->T', 'T->I'):\n",
    "        print(f'\n",
    "{direction}')\n",
    "        header = '{:<8} {:>12} {:>12} {:>12}'.format('Metric', 'Raw', 'Fine-tuned', 'Î” (pp)')\n",
    "        print(header)\n",
    "        print('-' * len(header))\n",
    "        for metric in ('R@1', 'R@5', 'R@10', 'MedR'):\n",
    "            base_val = float(baseline[direction][metric])\n",
    "            tuned_val = float(tuned[direction][metric])\n",
    "            delta = tuned_val - base_val\n",
    "            arrow = 'â†’'\n",
    "            if metric == 'MedR':\n",
    "                if delta < 0:\n",
    "                    arrow = 'â†“'\n",
    "                elif delta > 0:\n",
    "                    arrow = 'â†‘'\n",
    "            else:\n",
    "                if delta > 0:\n",
    "                    arrow = 'â†‘'\n",
    "                elif delta < 0:\n",
    "                    arrow = 'â†“'\n",
    "            print(\n",
    "                '{:<8} {:>12} {:>12} {:>5s}{:>6.2f}'.format(\n",
    "                    metric, fmt(base_val), fmt(tuned_val), arrow, abs(delta)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def _save_results_json(\n",
    "    path: str | os.PathLike[str],\n",
    "    val_raw: dict,\n",
    "    val_tuned: dict,\n",
    "    test_raw: dict,\n",
    "    test_tuned: dict,\n",
    ") -> None:\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    payload = {\n",
    "        'val': {'raw': val_raw, 'tuned': val_tuned},\n",
    "        'test': {'raw': test_raw, 'tuned': test_tuned},\n",
    "    }\n",
    "    with path.open('w', encoding='utf-8') as handle:\n",
    "        json.dump(payload, handle, indent=2)\n",
    "    print(f'\n",
    "Saved results to: {path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(\n",
    "    split: str,\n",
    "    preprocess,\n",
    "    tokenizer,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    "    subset: int,\n",
    "    data_csv: Optional[str],\n",
    "    torchvision_root: Optional[str],\n",
    "    torchvision_ann: Optional[str],\n",
    "    torchvision_split: Optional[str],\n",
    ") -> DataLoader:\n",
    "    if data_csv:\n",
    "        dataset = TSVFlickr(\n",
    "            data_csv,\n",
    "            split,\n",
    "            preprocess,\n",
    "            tokenizer,\n",
    "            subset=subset if subset > 0 else None,\n",
    "        )\n",
    "    elif torchvision_root and torchvision_ann:\n",
    "        split_file: Optional[Path] = None\n",
    "        if torchvision_split:\n",
    "            split_path = Path(torchvision_split)\n",
    "            if split_path.is_dir():\n",
    "                for suffix in ('.txt', '.lst', '.list'):\n",
    "                    candidate = split_path / f\"{split.lower()}{suffix}\"\n",
    "                    if candidate.is_file():\n",
    "                        split_file = candidate\n",
    "                        break\n",
    "            elif split_path.is_file():\n",
    "                split_file = split_path\n",
    "        dataset = TorchvisionFlickr(\n",
    "            torchvision_root,\n",
    "            torchvision_ann,\n",
    "            split,\n",
    "            preprocess,\n",
    "            tokenizer,\n",
    "            subset=subset if subset > 0 else None,\n",
    "            split_file=split_file,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('Provide either data_csv or torchvision root/annotation paths.')\n",
    "\n",
    "    workers = 2 if torch.get_num_threads() > 1 else 0\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=workers,\n",
    "        pin_memory=False,\n",
    "        collate_fn=collate,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a6770",
   "metadata": {},
   "source": [
    "## Configure Dataset Source\n",
    "\n",
    "- **Torchvision**: set `torchvision_root` to the image directory, `torchvision_ann`\n",
    "  to `captions.txt`, and (optionally) `torchvision_split` to a folder/file with\n",
    "  `train/val/test` lists. This is the default/official setup.\n",
    "- **TSV**: if you maintain a custom tab-separated manifest, provide its path via\n",
    "  `data_csv`. Leave the torchvision fields blank.\n",
    "\n",
    "Do not mix sources in the same run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eccf1d",
   "metadata": {},
   "source": [
    "## Download Flickr30k Dataset\n",
    "\n",
    "The Flickr30k dataset consists of images and captions. You need to download both components:\n",
    "\n",
    "### Method 1: Manual Download (Recommended)\n",
    "\n",
    "1. **Images**: \n",
    "   - Visit: https://shannon.cs.illinois.edu/DenotationGraph/\n",
    "   - Request access and download `flickr30k_images.tar.gz`\n",
    "   \n",
    "2. **Captions**: \n",
    "   - Download from: http://shannon.cs.illinois.edu/DenotationGraph/data/flickr30k.tar.gz\n",
    "\n",
    "### Method 2: Using Commands\n",
    "\n",
    "Run the download and setup commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ae99b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Flickr30k Dataset Status:\n",
      "========================================\n",
      "ðŸ” Looking in: /Users/yzqi/Projects/clip-finetuning/data\n",
      "ðŸ“ Data directory: âœ… Exists\n",
      "ðŸ“ Images directory: âœ… Exists\n",
      "ðŸ“„ Captions file: âœ… Exists\n",
      "ðŸ–¼ï¸  Image count: 0\n",
      "\n",
      "âš ï¸  No images found in /Users/yzqi/Projects/clip-finetuning/data/flickr30k_images\n",
      "   Please download flickr30k_images.tar.gz from:\n",
      "   https://shannon.cs.illinois.edu/DenotationGraph/\n",
      "   And extract all .jpg files to: /Users/yzqi/Projects/clip-finetuning/data/flickr30k_images\n",
      "\n",
      "==================================================\n",
      "ðŸ“¥ TO COMPLETE SETUP:\n",
      "1. Visit: https://shannon.cs.illinois.edu/DenotationGraph/\n",
      "2. Request access and download 'flickr30k_images.tar.gz'\n",
      "3. Extract to: ../data/flickr30k_images/ (relative to notebook)\n",
      "4. Run this cell again to verify setup\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Check Flickr30k dataset setup status\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def check_flickr30k_status():\n",
    "    \"\"\"Check the current status of Flickr30k dataset setup\"\"\"\n",
    "    \n",
    "    # Use absolute path to project root\n",
    "    project_root = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "    data_dir = project_root / 'data'\n",
    "    images_dir = data_dir / 'flickr30k_images'\n",
    "    caption_file = data_dir / 'flickr30k' / 'results_20130124.token'\n",
    "    \n",
    "    print(\"ðŸ“Š Flickr30k Dataset Status:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"ðŸ” Looking in: {data_dir}\")\n",
    "    \n",
    "    # Check directories\n",
    "    print(f\"ðŸ“ Data directory: {'âœ… Exists' if data_dir.exists() else 'âŒ Missing'}\")\n",
    "    print(f\"ðŸ“ Images directory: {'âœ… Exists' if images_dir.exists() else 'âŒ Missing'}\")\n",
    "    print(f\"ðŸ“„ Captions file: {'âœ… Exists' if caption_file.exists() else 'âŒ Missing'}\")\n",
    "    \n",
    "    # Count images if directory exists\n",
    "    if images_dir.exists():\n",
    "        img_count = len(list(images_dir.glob('*.jpg')))\n",
    "        print(f\"ðŸ–¼ï¸  Image count: {img_count}\")\n",
    "        \n",
    "        if img_count > 0:\n",
    "            print(\"\\nðŸŽ‰ Dataset ready for training!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸  No images found in {images_dir}\")\n",
    "            print(\"   Please download flickr30k_images.tar.gz from:\")\n",
    "            print(\"   https://shannon.cs.illinois.edu/DenotationGraph/\")\n",
    "            print(f\"   And extract all .jpg files to: {images_dir}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"\\nâŒ Images directory {images_dir} not found\")\n",
    "        return False\n",
    "\n",
    "# Run the status check\n",
    "dataset_ready = check_flickr30k_status()\n",
    "\n",
    "if not dataset_ready:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ“¥ TO COMPLETE SETUP:\")\n",
    "    print(\"1. Visit: https://shannon.cs.illinois.edu/DenotationGraph/\")\n",
    "    print(\"2. Request access and download 'flickr30k_images.tar.gz'\")\n",
    "    print(\"3. Extract to: ../data/flickr30k_images/ (relative to notebook)\")\n",
    "    print(\"4. Run this cell again to verify setup\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b96db",
   "metadata": {},
   "source": [
    "### Alternative: Terminal Commands\n",
    "\n",
    "If you prefer using the terminal, run these commands:\n",
    "\n",
    "```bash\n",
    "# Create directory structure\n",
    "mkdir -p ./data/flickr30k_images\n",
    "mkdir -p ./data/flickr30k\n",
    "\n",
    "# Download captions (annotations)\n",
    "cd ./data\n",
    "wget http://shannon.cs.illinois.edu/DenotationGraph/data/flickr30k.tar.gz\n",
    "tar -xzf flickr30k.tar.gz\n",
    "rm flickr30k.tar.gz\n",
    "\n",
    "# For images: manually download from https://shannon.cs.illinois.edu/DenotationGraph/\n",
    "# Extract flickr30k_images.tar.gz to ./data/flickr30k_images/\n",
    "```\n",
    "\n",
    "### Expected Final Structure:\n",
    "```\n",
    "./data/\n",
    "â”œâ”€â”€ flickr30k_images/\n",
    "â”‚   â”œâ”€â”€ 1000092795.jpg\n",
    "â”‚   â”œâ”€â”€ 10002456.jpg\n",
    "â”‚   â””â”€â”€ ... (31,783 total images)\n",
    "â””â”€â”€ flickr30k/\n",
    "    â””â”€â”€ results_20130124.token\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: If you have direct access to flickr30k_images.tar.gz, \n",
    "# uncomment and modify the path below to extract it automatically\n",
    "\n",
    "def extract_flickr30k_images(tar_path):\n",
    "    \"\"\"Extract Flickr30k images from tar file\"\"\"\n",
    "    import tarfile\n",
    "    \n",
    "    tar_path = Path(tar_path)\n",
    "    if not tar_path.exists():\n",
    "        print(f\"âŒ File not found: {tar_path}\")\n",
    "        return\n",
    "    \n",
    "    extract_dir = Path('./data/flickr30k_images')\n",
    "    extract_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"ðŸ“¦ Extracting {tar_path.name}...\")\n",
    "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "        # Extract only .jpg files\n",
    "        members = [m for m in tar.getmembers() if m.name.endswith('.jpg')]\n",
    "        for member in members:\n",
    "            # Extract with just the filename, not the full path\n",
    "            member.name = Path(member.name).name\n",
    "            tar.extract(member, extract_dir)\n",
    "            \n",
    "    print(f\"âœ… Extracted {len(members)} images to {extract_dir}\")\n",
    "\n",
    "# Uncomment and set the correct path if you have the tar file:\n",
    "# extract_flickr30k_images(\"path/to/flickr30k_images.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    data_csv: str = ''  # TSV path if using local data\n",
    "    torchvision_root: str = ''  # Directory with Flickr30k images\n",
    "    torchvision_ann: str = ''  # Path to captions.txt\n",
    "    torchvision_split: str = ''  # Optional split directory/file\n",
    "    epochs: int = 1\n",
    "    batch_size: int = 32\n",
    "    subset: int = 5000\n",
    "    zero_shot: bool = False\n",
    "    val_split: str = 'val'\n",
    "    test_split: str = 'test'\n",
    "    results_json: str = 'artifacts/clip_flickr30k_results.json'\n",
    "    save_checkpoint: bool = True\n",
    "    checkpoint_path: str = 'checkpoints/linearprobe_best.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36679a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(cfg: Config) -> dict:\n",
    "    device = 'cpu'\n",
    "    model_name = 'ViT-B-32'\n",
    "    pretrained = 'openai'\n",
    "\n",
    "    model, preprocess_train, preprocess_eval = open_clip.create_model_and_transforms(\n",
    "        model_name, pretrained=pretrained\n",
    "    )\n",
    "    tokenizer = open_clip.get_tokenizer(model_name)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if not cfg.data_csv and not (cfg.torchvision_root and cfg.torchvision_ann):\n",
    "        raise ValueError('Provide either data_csv or torchvision root/ann paths.')\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    trainable_params: List[torch.nn.Parameter] = []\n",
    "    if getattr(model, 'text_projection', None) is not None:\n",
    "        model.text_projection.requires_grad = True\n",
    "        trainable_params.append(model.text_projection)\n",
    "    if getattr(model, 'visual', None) is not None and getattr(model.visual, 'proj', None) is not None:\n",
    "        model.visual.proj.requires_grad = True\n",
    "        trainable_params.append(model.visual.proj)\n",
    "\n",
    "    model.logit_scale.requires_grad = True\n",
    "    trainable_params.append(model.logit_scale)\n",
    "\n",
    "    val_loader = build_dataloader(\n",
    "        cfg.val_split,\n",
    "        preprocess_eval,\n",
    "        tokenizer,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        subset=0,\n",
    "        data_csv=cfg.data_csv,\n",
    "        torchvision_root=cfg.torchvision_root,\n",
    "        torchvision_ann=cfg.torchvision_ann,\n",
    "        torchvision_split=cfg.torchvision_split,\n",
    "    )\n",
    "    test_loader = build_dataloader(\n",
    "        cfg.test_split,\n",
    "        preprocess_eval,\n",
    "        tokenizer,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        subset=0,\n",
    "        data_csv=cfg.data_csv,\n",
    "        torchvision_root=cfg.torchvision_root,\n",
    "        torchvision_ann=cfg.torchvision_ann,\n",
    "        torchvision_split=cfg.torchvision_split,\n",
    "    )\n",
    "\n",
    "    print('Evaluating RAW CLIP (zero-shot) before training...')\n",
    "    baseline_val = evaluate(model, val_loader, device)\n",
    "    baseline_test = evaluate(model, test_loader, device)\n",
    "    _compare_and_print('VAL', baseline_val, baseline_val)\n",
    "    _compare_and_print('TEST', baseline_test, baseline_test)\n",
    "\n",
    "    if cfg.zero_shot:\n",
    "        _save_results_json(\n",
    "            cfg.results_json,\n",
    "            _flatten_metrics(baseline_val),\n",
    "            _flatten_metrics(baseline_val),\n",
    "            _flatten_metrics(baseline_test),\n",
    "            _flatten_metrics(baseline_test),\n",
    "        )\n",
    "        return {\n",
    "            'baseline': {'val': baseline_val, 'test': baseline_test},\n",
    "            'fine_tuned': {'val': baseline_val, 'test': baseline_test},\n",
    "        }\n",
    "\n",
    "    train_loader = build_dataloader(\n",
    "        'train',\n",
    "        preprocess_train,\n",
    "        tokenizer,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        subset=cfg.subset,\n",
    "        data_csv=cfg.data_csv,\n",
    "        torchvision_root=cfg.torchvision_root,\n",
    "        torchvision_ann=cfg.torchvision_ann,\n",
    "        torchvision_split=cfg.torchvision_split,\n",
    "    )\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        [param for param in trainable_params if param is not None],\n",
    "        lr=1e-3,\n",
    "        weight_decay=0.2,\n",
    "    )\n",
    "\n",
    "    best_val_r1 = -float('inf')\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        print(f'Epoch {epoch}/{cfg.epochs}')\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        val_score = 0.5 * (\n",
    "            val_metrics['I->T']['R@1'] + val_metrics['T->I']['R@1']\n",
    "        )\n",
    "        print(\n",
    "            f'train_loss={train_loss:.4f}  val_R@1(avg)={val_score:.2f}  details={val_metrics}'\n",
    "        )\n",
    "\n",
    "        if cfg.save_checkpoint and val_score > best_val_r1:\n",
    "            best_val_r1 = val_score\n",
    "            Path(cfg.checkpoint_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save({'model': model.state_dict()}, cfg.checkpoint_path)\n",
    "            print(f'Saved checkpoint to {cfg.checkpoint_path}')\n",
    "\n",
    "    print('\n",
    "Running final evaluations with fine-tuned model...')\n",
    "    val_metrics = evaluate(model, val_loader, device)\n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "\n",
    "    _compare_and_print('VAL', baseline_val, val_metrics)\n",
    "    _compare_and_print('TEST', baseline_test, test_metrics)\n",
    "\n",
    "    _save_results_json(\n",
    "        cfg.results_json,\n",
    "        _flatten_metrics(baseline_val),\n",
    "        _flatten_metrics(val_metrics),\n",
    "        _flatten_metrics(baseline_test),\n",
    "        _flatten_metrics(test_metrics),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'baseline': {'val': baseline_val, 'test': baseline_test},\n",
    "        'fine_tuned': {'val': val_metrics, 'test': test_metrics},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ee014",
   "metadata": {},
   "source": [
    "\n",
    "## Recommended CPU-Friendly Settings\n",
    "\n",
    "Start with a quick sanity run on a subset before committing to the full train\n",
    "split:\n",
    "\n",
    "- `epochs = 1`\n",
    "- `batch_size = 32`\n",
    "- `subset = 5000`\n",
    "\n",
    "If the run is still slow, drop `subset` to ~3000. For improved accuracy once\n",
    "things look stable, try `epochs = 3` (still CPU-only, just longer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eba291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration (edit and uncomment to run)\n",
    "# cfg = Config(\n",
    "#     data_csv='',                 # set if using TSV\n",
    "#     torchvision_root='/data/flickr30k/images',\n",
    "#     torchvision_ann='/data/flickr30k/annotations/captions.txt',\n",
    "#     torchvision_split='/data/flickr30k/splits',\n",
    "#     epochs=1,\n",
    "#     batch_size=32,\n",
    "#     subset=5000,\n",
    "#     zero_shot=False,\n",
    "# )\n",
    "# results = run_experiment(cfg)\n",
    "# results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45d575",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Next Steps\n",
    "\n",
    "- The JSON metrics file (default `artifacts/clip_flickr30k_results.json`) stores\n",
    "  both baseline and fine-tuned metrics for later analysis or visualization.\n",
    "- Use `zero_shot=True` to capture just the raw CLIP performance before any\n",
    "  training. This is useful for regression checks.\n",
    "- The notebook's helper functions mirror the standalone scripts\n",
    "  (`finetune_clip_flickr30k_cpu.py` and `scripts/compare_models.py`) so you can\n",
    "  mix and match CLI or notebook workflows depending on preference.\n",
    "- Consider logging additional diagnostics (loss curves, wall-clock timings) or\n",
    "  exporting metrics to experiment trackers as you iterate.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
